# å£°éŸ³åˆ†ç±»ç³»ç»Ÿ (Sound Classification System)

åŸºäº Audio Spectrogram Transformer (AST) çš„é¡¾å®¢è¡Œä¸ºå£°éŸ³åˆ†ç±»ç³»ç»Ÿï¼Œç”¨äºè¯†åˆ«å’Œåˆ†æé¡¾å®¢åœ¨å•†ä¸šç¯å¢ƒä¸­çš„æƒ…ç»ªå’Œè¡Œä¸ºçŠ¶æ€ã€‚

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®æ—¨åœ¨é€šè¿‡åˆ†æå£°éŸ³ä¿¡å·æ¥è¯†åˆ«é¡¾å®¢çš„æƒ…ç»ªçŠ¶æ€å’Œè¡Œä¸ºæ¨¡å¼ï¼Œå¸®åŠ©å•†å®¶æ›´å¥½åœ°ç†è§£é¡¾å®¢ä½“éªŒã€‚ç³»ç»Ÿèƒ½å¤Ÿè¯†åˆ«ä»¥ä¸‹7ç§å£°éŸ³ç±»å‹ï¼š

| å£°éŸ³ç±»å‹ | ä¸­æ–‡åç§° | æƒ…ç»ªç±»åˆ« | å•†ä¸šå«ä¹‰ |
|---------|---------|---------|---------|
| `laughing` | ç¬‘å£° | ç§¯æ | é¡¾å®¢æ»¡æ„åº¦é«˜ï¼Œä½“éªŒè‰¯å¥½ |
| `sighing` | å¹æ°” | æ¶ˆæ | ç­‰å¾…è¿‡ä¹…æˆ–ä¸æ»¡æƒ…ç»ª |
| `tongue_clicking` | å’‚èˆŒ | æ¶ˆæ | å¯¹æœåŠ¡ä¸æ»¡ï¼Œè¡¨è¾¾ç„¦èº |
| `throat_clearing` | æ¸…å—“ | ä¸­æ€§ | ç­‰å¾…ä¸­çš„ç´§å¼ æˆ–å°è¯•å¼•èµ·æ³¨æ„ |
| `teeth_grinding` | ç£¨ç‰™ | æ¶ˆæ | å¼ºçƒˆæŒ«è´¥æ„Ÿæˆ–æ„¤æ€’ |
| `yawning` | æ‰“å“ˆæ¬  | ä¸­æ€§ | åŒçƒ¦æˆ–å¯¹ä½“éªŒä¸æ„Ÿå…´è¶£ |
| `lip_smacking` | æŠ¿å˜´/å’‚å˜´ | ç§¯æ | æœŸå¾…æˆ–è½»åº¦æ„‰æ‚¦ |

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒæŠ€æœ¯
- **æ¨¡å‹**: Audio Spectrogram Transformer (AST) - å½“å‰éŸ³é¢‘åˆ†ç±»çš„ SOTA æ¨¡å‹
- **å¤‡é€‰æ¨¡å‹**: è‡ªå®šä¹‰ CNN æ¨¡å‹ï¼ˆç”¨äºèµ„æºå—é™ç¯å¢ƒï¼‰
- **ç‰¹å¾æå–**: Melé¢‘è°±å›¾ + éŸ³é¢‘å¢å¼º
- **æ¡†æ¶**: PyTorch + Transformers

### é¡¹ç›®ç»“æ„
```
FINAL/
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶å’Œå‚æ•°è®¾ç½®
â”œâ”€â”€ data_preprocessing.py   # æ•°æ®é¢„å¤„ç†å’Œå¢å¼º
â”œâ”€â”€ model.py               # æ¨¡å‹å®šä¹‰ï¼ˆAST + CNNï¼‰
â”œâ”€â”€ train.py               # åŸºç¡€è®­ç»ƒè„šæœ¬
â”œâ”€â”€ train_real_data.py     # ğŸ†• çœŸå®æ•°æ®ä¸“ç”¨è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference.py           # æ¨ç†è„šæœ¬
â”œâ”€â”€ data_generator.py      # åˆæˆæ•°æ®ç”Ÿæˆå™¨
â”œâ”€â”€ organize_data.py       # ğŸ†• æ•°æ®ç»„ç»‡å’Œåˆ†æå·¥å…·
â”œâ”€â”€ requirements.txt       # ä¾èµ–åŒ…åˆ—è¡¨
â”œâ”€â”€ README.md             # é¡¹ç›®è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ data/                 # æ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ laughing/         # ç¬‘å£°æ ·æœ¬
â”‚   â”œâ”€â”€ sighing/          # å¹æ°”æ ·æœ¬
â”‚   â”œâ”€â”€ tongue_clicking/  # å’‚èˆŒæ ·æœ¬
â”‚   â”œâ”€â”€ throat_clearing/  # æ¸…å—“æ ·æœ¬
â”‚   â”œâ”€â”€ teeth_grinding/   # ç£¨ç‰™æ ·æœ¬
â”‚   â”œâ”€â”€ yawning/          # æ‰“å“ˆæ¬ æ ·æœ¬
â”‚   â””â”€â”€ lip_smacking/     # æŠ¿å˜´æ ·æœ¬
â”œâ”€â”€ models/               # ä¿å­˜çš„æ¨¡å‹
â””â”€â”€ logs/                # è®­ç»ƒæ—¥å¿—å’Œç»“æœ
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒè®¾ç½®

```bash
# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# æˆ–ä½¿ç”¨conda
conda create -n sound_classification python=3.8
conda activate sound_classification
pip install -r requirements.txt
```

### 2. æ•°æ®å‡†å¤‡

#### é€‰é¡¹A: ä½¿ç”¨çœŸå®æ•°æ®ï¼ˆæ¨èï¼‰
å°†éŸ³é¢‘æ–‡ä»¶æŒ‰ç±»åˆ«æ”¾å…¥ç›¸åº”ç›®å½•ï¼š
```
data/
â”œâ”€â”€ laughing/          # æˆ– laughing-sounds/ ç­‰å˜ä½“
â”‚   â”œâ”€â”€ real_laugh_001.wav
â”‚   â””â”€â”€ real_laugh_002.wav
â”œâ”€â”€ sighing/           # æˆ– sighing-sounds/ ç­‰å˜ä½“
â”‚   â”œâ”€â”€ real_sigh_001.wav
â”‚   â””â”€â”€ real_sigh_002.wav
â””â”€â”€ ...
```

**æ”¯æŒçš„éŸ³é¢‘æ ¼å¼**: WAV, MP3, FLAC, M4A, OGG
**ç›®å½•å‘½å**: æ”¯æŒä¸‹åˆ’çº¿ã€è¿å­—ç¬¦ç­‰å˜ä½“ï¼ˆå¦‚ `tongue-clicking` æˆ– `tongue_clicking`ï¼‰

#### é€‰é¡¹B: è‡ªåŠ¨æ•°æ®ç»„ç»‡
å¦‚æœä½ çš„æ•°æ®ç›®å½•ä½¿ç”¨ä¸åŒçš„å‘½åæ ¼å¼ï¼š

```bash
# è‡ªåŠ¨é‡å‘½åç›®å½•å¹¶åˆ†ææ•°æ®
py organize_data.py

# ä»…åˆ†ææ•°æ®åˆ†å¸ƒ
py organize_data.py --analyze_only

# ä»…æ¸…ç†æ— æ•ˆæ–‡ä»¶
py organize_data.py --clean_only
```

#### é€‰é¡¹C: ä½¿ç”¨åˆæˆæ•°æ®ï¼ˆæµ‹è¯•ç”¨ï¼‰
```bash
# ç”Ÿæˆåˆæˆè®­ç»ƒæ•°æ®
py data_generator.py
```

### 3. æ¨¡å‹è®­ç»ƒ

#### çœŸå®æ•°æ®è®­ç»ƒï¼ˆæ¨èï¼‰
```bash
# ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒï¼ˆé»˜è®¤30ä¸ªepochï¼‰
py train_real_data.py

# è‡ªå®šä¹‰å‚æ•°
py train_real_data.py --epochs 50 --batch_size 16 --learning_rate 1e-4

# ä½¿ç”¨Weights & Biasesè®°å½•
py train_real_data.py --use_wandb

# ä½¿ç”¨CNNæ¨¡å‹ï¼ˆå¦‚æœASTä¸å¯ç”¨ï¼‰
py train_real_data.py --model_type cnn
```

#### åŸºç¡€è®­ç»ƒ
```bash
# ä¼ ç»Ÿè®­ç»ƒæ–¹å¼
py train.py
```

### 4. æ¨¡å‹æ¨ç†

```bash
# ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹
py inference.py --model_path models/best_real_data_model.pth --input audio_file.wav

# å¯¹ç›®å½•ä¸­æ‰€æœ‰æ–‡ä»¶æ‰¹é‡é¢„æµ‹
py inference.py --model_path models/best_real_data_model.pth --input audio_directory/ --visualize

# ç”Ÿæˆå¯è§†åŒ–ç»“æœ
py inference.py --model_path models/best_real_data_model.pth --input data/ --output results.json --visualize --viz_output visualization.png
```

## ğŸ“Š æ•°æ®é›†ä¿¡æ¯

### æ”¯æŒçš„æ•°æ®æ ¼å¼
- **æ–‡ä»¶æ ¼å¼**: WAV, MP3, FLAC, M4A, OGG
- **é‡‡æ ·ç‡**: è‡ªåŠ¨è½¬æ¢ä¸º16kHz
- **æ—¶é•¿**: è‡ªåŠ¨è°ƒæ•´ä¸º5ç§’ï¼ˆæˆªå–æˆ–å¡«å……ï¼‰
- **é€šé“**: è‡ªåŠ¨è½¬æ¢ä¸ºå•å£°é“

### ç›®å½•ç»“æ„çµæ´»æ€§
ç³»ç»Ÿè‡ªåŠ¨è¯†åˆ«å¤šç§ç›®å½•å‘½åæ ¼å¼ï¼š
- æ ‡å‡†æ ¼å¼: `tongue_clicking`, `throat_clearing`
- è¿å­—ç¬¦æ ¼å¼: `tongue-clicking`, `throat-clearing`  
- å…¶ä»–å˜ä½“: `tongueclicking`, `tongue clicking`

### æ•°æ®ç»„ç»‡å·¥å…·
```bash
# å®Œæ•´æ•°æ®ç»„ç»‡æµç¨‹
py organize_data.py

# åˆ†æå½“å‰æ•°æ®çŠ¶æ€
py organize_data.py --analyze_only
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
ğŸ“Š Data distribution analysis:
  Total files: 313
  Real data files: 313
  Synthetic data files: 0

  Class distribution:
    laughing: 25 files (8.0%)
    sighing: 51 files (16.3%)
    tongue_clicking: 47 files (15.0%)
    throat_clearing: 54 files (17.3%)
    teeth_grinding: 43 files (13.7%)
    yawning: 48 files (15.3%)
    lip_smacking: 45 files (14.4%)

  âœ“ Dataset is reasonably balanced (ratio: 2.2)
```

## ğŸ“ è®­ç»ƒé…ç½®

### çœŸå®æ•°æ®è®­ç»ƒä¼˜åŒ–
`train_real_data.py` ä¸“é—¨ä¸ºçœŸå®æ•°æ®ä¼˜åŒ–ï¼š

- **å­¦ä¹ ç‡è°ƒåº¦**: ReduceLROnPlateauï¼ˆæ›´ä¿å®ˆï¼‰
- **æ—©åœç­–ç•¥**: 15ä¸ªepochè€å¿ƒï¼ˆæ›´å®½å®¹ï¼‰
- **ä¼˜åŒ–å™¨**: å¯¹ASTé¢„è®­ç»ƒå±‚ä½¿ç”¨æ›´ä½å­¦ä¹ ç‡
- **æ•°æ®å¢å¼º**: é€‚åˆçœŸå®æ•°æ®çš„å¢å¼ºç­–ç•¥
- **é”™è¯¯å¤„ç†**: æ›´å¥½çš„æ‰¹å¤„ç†é”™è¯¯æ¢å¤

### é»˜è®¤é…ç½®
- **æ¨¡å‹**: MIT/ast-finetuned-audioset-10-10-0.4593
- **éŸ³é¢‘å‚æ•°**: 16kHzé‡‡æ ·ç‡ï¼Œ5ç§’æ—¶é•¿ï¼Œ128ä¸ªMelé¢‘å¸¦
- **è®­ç»ƒå‚æ•°**: 
  - æ‰¹å¤§å°: 16
  - å­¦ä¹ ç‡: 1e-4ï¼ˆASTå±‚ï¼š1e-5ï¼‰
  - ä¼˜åŒ–å™¨: AdamW
  - è°ƒåº¦å™¨: ReduceLROnPlateau
  - æ—©åœ: 15ä¸ªepoch

### æ•°æ®å¢å¼º
- é«˜æ–¯å™ªå£°æ·»åŠ 
- æ—¶é—´æ‹‰ä¼¸ (0.8x - 1.25x)
- éŸ³è°ƒåç§» (Â±4åŠéŸ³)
- æ—¶é—´åç§»

## ğŸ› ï¸ é«˜çº§ä½¿ç”¨

### è‡ªå®šä¹‰é…ç½®

ç¼–è¾‘ `config.py` ä¸­çš„å‚æ•°ï¼š

```python
class Config:
    # éŸ³é¢‘å‚æ•°
    sample_rate: int = 16000
    max_audio_length: int = 5
    
    # æ¨¡å‹å‚æ•°  
    model_name: str = "MIT/ast-finetuned-audioset-10-10-0.4593"
    num_classes: int = 7
    
    # è®­ç»ƒå‚æ•°
    batch_size: int = 16
    learning_rate: float = 1e-4
    num_epochs: int = 30  # çœŸå®æ•°æ®æ¨è30-50ä¸ªepoch
```

### æ·»åŠ æ–°çš„å£°éŸ³ç±»åˆ«

1. åœ¨ `config.py` ä¸­æ·»åŠ æ–°ç±»åˆ«åç§°
2. åœ¨ `SOUND_DESCRIPTIONS` å’Œ `EMOTION_MAPPING` ä¸­æ·»åŠ æè¿°
3. åˆ›å»ºå¯¹åº”çš„æ•°æ®ç›®å½•å¹¶æ·»åŠ éŸ³é¢‘æ ·æœ¬
4. è¿è¡Œ `py organize_data.py` æ£€æŸ¥æ•°æ®
5. é‡æ–°è®­ç»ƒæ¨¡å‹

### ä½¿ç”¨ä¸åŒçš„æ¨¡å‹

```python
# åœ¨ train_real_data.py ä¸­åˆ‡æ¢æ¨¡å‹
py train_real_data.py --model_type cnn   # ä½¿ç”¨CNNæ¨¡å‹
py train_real_data.py --model_type ast   # ä½¿ç”¨ASTæ¨¡å‹ï¼ˆé»˜è®¤ï¼‰
```

## ğŸ“ˆ ç›‘æ§å’Œå¯è§†åŒ–

### è®­ç»ƒç›‘æ§
- è®­ç»ƒ/éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡æ›²çº¿
- å­¦ä¹ ç‡å˜åŒ–
- æ··æ·†çŸ©é˜µï¼ˆå¸¦ç™¾åˆ†æ¯”ï¼‰
- åˆ†ç±»æŠ¥å‘Š
- çœŸå®æ•°æ®vsåˆæˆæ•°æ®ç»Ÿè®¡

### æ¨ç†ç»“æœå¯è§†åŒ–
- é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ
- ç½®ä¿¡åº¦åˆ†å¸ƒ  
- æƒ…ç»ªåˆ†ç±»é¥¼å›¾
- å„ç±»åˆ«ç½®ä¿¡åº¦ç®±çº¿å›¾

### è¾“å‡ºæ–‡ä»¶
è®­ç»ƒå®Œæˆåæ£€æŸ¥ï¼š
- `models/best_real_data_model.pth` - æœ€ä½³æ¨¡å‹
- `logs/real_data_confusion_matrix.png` - æ··æ·†çŸ©é˜µ
- `logs/real_data_training_history.png` - è®­ç»ƒæ›²çº¿
- `logs/real_data_test_results.json` - è¯¦ç»†æµ‹è¯•ç»“æœ

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ•°æ®ç›®å½•åç§°ä¸åŒ¹é…**
   ```bash
   # è‡ªåŠ¨ä¿®å¤ç›®å½•å‘½å
   py organize_data.py
   ```

2. **CUDAå†…å­˜ä¸è¶³**
   ```python
   # å‡å°‘æ‰¹å¤§å°
   py train_real_data.py --batch_size 8
   
   # æˆ–ä½¿ç”¨CPU
   device = torch.device("cpu")
   ```

3. **éŸ³é¢‘æ–‡ä»¶åŠ è½½å¤±è´¥**
   ```bash
   # æ¸…ç†æ— æ•ˆæ–‡ä»¶
   py organize_data.py --clean_only
   ```

4. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   ```bash
   # ä½¿ç”¨CNNæ¨¡å‹
   py train_real_data.py --model_type cnn
   ```

5. **æ•°æ®ä¸å¹³è¡¡**
   ```bash
   # æ£€æŸ¥æ•°æ®åˆ†å¸ƒ
   py organize_data.py --analyze_only
   
   # ä¸ºç¼ºå°‘çš„ç±»åˆ«ç”Ÿæˆåˆæˆæ•°æ®
   py data_generator.py --classes tongue_clicking lip_smacking
   ```

### æ€§èƒ½ä¼˜åŒ–å»ºè®®

æ ¹æ®æ•°æ®é›†å¤§å°é€‰æ‹©é…ç½®ï¼š

- **å°æ•°æ®é›† (<100æ ·æœ¬/ç±»åˆ«)**:
  ```bash
  py train_real_data.py --batch_size 4 --epochs 50 --learning_rate 5e-5
  ```

- **ä¸­ç­‰æ•°æ®é›† (100-500æ ·æœ¬/ç±»åˆ«)**:
  ```bash
  py train_real_data.py --batch_size 16 --epochs 30 --learning_rate 1e-4
  ```

- **å¤§æ•°æ®é›† (>500æ ·æœ¬/ç±»åˆ«)**:
  ```bash
  py train_real_data.py --batch_size 32 --epochs 50 --learning_rate 1e-4 --use_wandb
  ```

## ğŸ“ APIå‚è€ƒ

### SoundClassifierç±»

```python
from inference import SoundClassifier

# åˆå§‹åŒ–åˆ†ç±»å™¨ï¼ˆä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒçš„æ¨¡å‹ï¼‰
classifier = SoundClassifier("models/best_real_data_model.pth")

# å•æ–‡ä»¶é¢„æµ‹
result = classifier.predict_single("audio.wav")
print(f"ç±»åˆ«: {result['predicted_class']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")

# æ‰¹é‡é¢„æµ‹
results = classifier.predict_batch(["audio1.wav", "audio2.wav"])

# ç›®å½•é¢„æµ‹
results = classifier.predict_directory("audio_directory/")
```

### æ•°æ®ç»„ç»‡å·¥å…·

```python
from organize_data import analyze_data_distribution

# åˆ†ææ•°æ®åˆ†å¸ƒ
analyze_data_distribution("data/")

# è‡ªåŠ¨é‡å‘½åç›®å½•
from organize_data import rename_directories
rename_directories("data/")
```

### è¾“å‡ºæ ¼å¼

```json
{
  "audio_path": "path/to/audio.wav",
  "predicted_class": "laughing",
  "confidence": 0.924,
  "emotion": "positive", 
  "description": "é¡¾å®¢æ»¡æ„æˆ–äº«å—ä½“éªŒæ—¶å¸¸ä¼šå‘å‡ºçš„æ­£å‘åé¦ˆ",
  "all_probabilities": {
    "laughing": 0.924,
    "sighing": 0.041,
    "tongue_clicking": 0.018,
    "throat_clearing": 0.012,
    "teeth_grinding": 0.003,
    "yawning": 0.001,
    "lip_smacking": 0.001
  }
}
```

## ğŸ“Š å®é™…æ•°æ®ç»“æœ

åŸºäºçœŸå®éŸ³é¢‘æ•°æ®çš„æµ‹è¯•ç»“æœï¼š

- **æ•°æ®é›†è§„æ¨¡**: 313ä¸ªçœŸå®éŸ³é¢‘æ–‡ä»¶
- **ç±»åˆ«å¹³è¡¡åº¦**: 2.2ï¼ˆä¼˜ç§€ï¼‰
- **å»ºè®®è®­ç»ƒé…ç½®**: ä¸­ç­‰è®¾ç½®ï¼ˆbatch_size=16, epochs=30-50ï¼‰
- **é¢„æœŸå‡†ç¡®ç‡**: 80-95%ï¼ˆå–å†³äºæ•°æ®è´¨é‡ï¼‰

## ğŸ¤ è´¡çŒ®æŒ‡å—

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- [Audio Spectrogram Transformer](https://github.com/YuanGongND/ast) - æ ¸å¿ƒæ¨¡å‹æ¶æ„
- [Hugging Face Transformers](https://huggingface.co/transformers/) - é¢„è®­ç»ƒæ¨¡å‹
- [librosa](https://librosa.org/) - éŸ³é¢‘å¤„ç†åº“

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- åˆ›å»º [Issue](https://github.com/your-repo/issues)
- å‘é€é‚®ä»¶åˆ°: your-email@example.com

---

â­ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ªæ˜Ÿæ ‡ï¼ 